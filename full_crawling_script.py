#!/usr/bin/env python3
"""
ì‹œì‚¬ì˜¤ëŠ˜ ì „ì²´ ì¹´í…Œê³ ë¦¬ ì™„ì „ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
"""

import os
import sys
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from sisaon_crawler_with_ranking import SisaonCrawler, JournalistRankingSystem
from database_manager import db_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('full_crawling.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FullCrawlingManager:
    """ì „ì²´ í¬ë¡¤ë§ ê´€ë¦¬ì"""
    
    def __init__(self, max_workers: int = 5, delay_between_categories: int = 10):
        self.max_workers = max_workers
        self.delay_between_categories = delay_between_categories
        
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ['DB_HOST'] = 'localhost'
        os.environ['DB_PORT'] = '5432'
        os.environ['DB_NAME'] = 'postgres'
        os.environ['DB_USER'] = 'postgres'
        # os.environ['DB_PASSWORD'] = 'your_password_here'  # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ ë³€ê²½í•˜ì„¸ìš”
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        self.crawler = SisaonCrawler(max_workers=max_workers)
        self.ranking_system = JournalistRankingSystem()
        
        # ì „ì²´ í†µê³„
        self.total_stats = {
            'start_time': None,
            'end_time': None,
            'total_articles': 0,
            'total_categories': 0,
            'total_pages': 0,
            'successful_categories': 0,
            'failed_categories': [],
            'category_details': {},
            'errors': 0,
            'duplicates': 0
        }
    
    def estimate_total_pages(self) -> Dict[str, int]:
        """ê° ì¹´í…Œê³ ë¦¬ì˜ ì´ í˜ì´ì§€ ìˆ˜ ì¶”ì •"""
        logger.info("ê° ì¹´í…Œê³ ë¦¬ì˜ ì´ í˜ì´ì§€ ìˆ˜ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...")
        
        category_pages = {}
        total_estimated_pages = 0
        
        for category, code in self.crawler.categories.items():
            try:
                category_url = f"{self.crawler.base_url}/news/articleList.html?sc_sub_section_code={code}&view_type=sm"
                total_pages = self.crawler.get_total_pages(category_url)
                category_pages[category] = total_pages
                total_estimated_pages += total_pages
                
                logger.info(f"  {category}: {total_pages}í˜ì´ì§€")
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"  {category} í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
                category_pages[category] = 0
        
        logger.info(f"ì´ ì˜ˆìƒ í˜ì´ì§€ ìˆ˜: {total_estimated_pages}í˜ì´ì§€")
        return category_pages
    
    def crawl_single_category(self, category: str, category_code: str, max_pages: int = None) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì‹œì‘...")
        
        category_start_time = datetime.now()
        category_stats = {
            'category': category,
            'start_time': category_start_time,
            'end_time': None,
            'articles_count': 0,
            'pages_crawled': 0,
            'errors': 0,
            'duplicates': 0,
            'success': False
        }
        
        try:
            # ì¹´í…Œê³ ë¦¬ URL ìƒì„±
            category_url = f"{self.crawler.base_url}/news/articleList.html?sc_sub_section_code={category_code}&view_type=sm"
            
            # ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸
            total_pages = self.crawler.get_total_pages(category_url)
            if max_pages:
                total_pages = min(total_pages, max_pages)
            
            logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}': ì´ {total_pages}í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì •")
            
            # í˜ì´ì§€ë³„ í¬ë¡¤ë§
            for page in range(1, total_pages + 1):
                try:
                    page_url = f"{category_url}&page={page}"
                    logger.info(f"  í˜ì´ì§€ {page}/{total_pages} í¬ë¡¤ë§ ì¤‘... ({page/total_pages*100:.1f}%)")
                    
                    # í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                    article_links = self.crawler.get_article_links_from_page(page_url)
                    
                    if not article_links:
                        logger.warning(f"  í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    
                    logger.info(f"  í˜ì´ì§€ {page}ì—ì„œ {len(article_links)}ê°œ ê¸°ì‚¬ ë§í¬ ë°œê²¬")
                    
                    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ í¬ë¡¤ë§
                    successful_articles = 0
                    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                        future_to_url = {
                            executor.submit(self.crawler._process_article, article_url, category): article_url 
                            for article_url in article_links
                        }
                        
                        for future in as_completed(future_to_url):
                            article_url = future_to_url[future]
                            try:
                                result = future.result(timeout=60)
                                if result:
                                    successful_articles += 1
                                    category_stats['articles_count'] += 1
                                    self.total_stats['total_articles'] += 1
                            except Exception as e:
                                logger.error(f"  ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ {article_url}: {e}")
                                category_stats['errors'] += 1
                                self.total_stats['errors'] += 1
                    
                    category_stats['pages_crawled'] += 1
                    self.total_stats['total_pages'] += 1
                    
                    logger.info(f"  í˜ì´ì§€ {page} ì™„ë£Œ: {successful_articles}/{len(article_links)} ê¸°ì‚¬ ì„±ê³µ")
                    
                    # í˜ì´ì§€ ê°„ê²© ì¡°ì ˆ
                    self.crawler._random_delay()
                    
                except Exception as e:
                    logger.error(f"  í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    category_stats['errors'] += 1
                    self.total_stats['errors'] += 1
                    continue
            
            category_stats['success'] = True
            self.total_stats['successful_categories'] += 1
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            category_stats['errors'] += 1
            self.total_stats['errors'] += 1
            self.total_stats['failed_categories'].append(category)
        
        finally:
            category_stats['end_time'] = datetime.now()
            category_duration = (category_stats['end_time'] - category_stats['start_time']).total_seconds()
            
            logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì™„ë£Œ:")
            logger.info(f"  - ê¸°ì‚¬ ìˆ˜: {category_stats['articles_count']}ê°œ")
            logger.info(f"  - í˜ì´ì§€ ìˆ˜: {category_stats['pages_crawled']}í˜ì´ì§€")
            logger.info(f"  - ì˜¤ë¥˜ ìˆ˜: {category_stats['errors']}ê°œ")
            logger.info(f"  - ì†Œìš” ì‹œê°„: {category_duration:.2f}ì´ˆ")
            
            self.total_stats['category_details'][category] = category_stats
        
        return category_stats
    
    def crawl_all_categories(self, max_pages_per_category: int = None, 
                           skip_categories: List[str] = None) -> bool:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        logger.info("ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        self.total_stats['start_time'] = datetime.now()
        self.total_stats['total_categories'] = len(self.crawler.categories)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        if not db_manager.initialize_pool():
            logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return False
        
        # í…Œì´ë¸” ìƒì„± í™•ì¸
        db_manager.create_tables()
        
        # ì´ í˜ì´ì§€ ìˆ˜ ì¶”ì •
        category_pages = self.estimate_total_pages()
        
        # í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ê²°ì •
        categories_to_crawl = {}
        for category, code in self.crawler.categories.items():
            if skip_categories and category in skip_categories:
                logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' ê±´ë„ˆë›°ê¸°")
                continue
            categories_to_crawl[category] = code
        
        logger.info(f"í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(categories_to_crawl)}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìˆœì°¨ í¬ë¡¤ë§
        for i, (category, code) in enumerate(categories_to_crawl.items(), 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ì¹´í…Œê³ ë¦¬ {i}/{len(categories_to_crawl)}: {category}")
            logger.info(f"{'='*60}")
            
            # ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
            category_stats = self.crawl_single_category(category, code, max_pages_per_category)
            
            # ì¹´í…Œê³ ë¦¬ ê°„ê²© ì¡°ì ˆ
            if i < len(categories_to_crawl):
                logger.info(f"ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ê¹Œì§€ {self.delay_between_categories}ì´ˆ ëŒ€ê¸°...")
                time.sleep(self.delay_between_categories)
        
        self.total_stats['end_time'] = datetime.now()
        self.print_final_summary()
        
        return True
    
    def print_final_summary(self):
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        duration = (self.total_stats['end_time'] - self.total_stats['start_time']).total_seconds()
        
        print("\n" + "="*80)
        print("ğŸ‰ ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*80)
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  - ì´ ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ ({duration/3600:.2f}ì‹œê°„)")
        print(f"  - ì´ ê¸°ì‚¬ ìˆ˜: {self.total_stats['total_articles']}ê°œ")
        print(f"  - ì´ í˜ì´ì§€ ìˆ˜: {self.total_stats['total_pages']}í˜ì´ì§€")
        print(f"  - ì„±ê³µí•œ ì¹´í…Œê³ ë¦¬: {self.total_stats['successful_categories']}/{self.total_stats['total_categories']}ê°œ")
        print(f"  - ì´ ì˜¤ë¥˜ ìˆ˜: {self.total_stats['errors']}ê°œ")
        print(f"  - ì¤‘ë³µ ê¸°ì‚¬ ìˆ˜: {self.total_stats['duplicates']}ê°œ")
        
        if self.total_stats['failed_categories']:
            print(f"  - ì‹¤íŒ¨í•œ ì¹´í…Œê³ ë¦¬: {', '.join(self.total_stats['failed_categories'])}")
        
        print(f"\nğŸ“ˆ ì„±ëŠ¥ í†µê³„:")
        if duration > 0:
            print(f"  - í‰ê·  ì²˜ë¦¬ ì†ë„: {self.total_stats['total_articles']/duration*60:.1f}ê°œ/ë¶„")
            print(f"  - í˜ì´ì§€ë‹¹ í‰ê·  ì‹œê°„: {duration/self.total_stats['total_pages']:.2f}ì´ˆ")
        
        print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ê²°ê³¼:")
        print("-" * 80)
        for category, stats in self.total_stats['category_details'].items():
            if stats['success']:
                duration = (stats['end_time'] - stats['start_time']).total_seconds()
                print(f"âœ… {category:<15}: {stats['articles_count']:4d}ê°œ ê¸°ì‚¬, {stats['pages_crawled']:3d}í˜ì´ì§€, {duration:6.1f}ì´ˆ")
            else:
                print(f"âŒ {category:<15}: ì‹¤íŒ¨")
        
        # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
        if hasattr(db_manager, 'connection_pool') and db_manager.connection_pool:
            db_manager.log_crawling_job(
                job_type='full_sisaon_crawling',
                source_key='ì‹œì‚¬ì˜¤ëŠ˜',
                status='completed',
                articles_count=self.total_stats['total_articles'],
                duration_seconds=duration,
                duplicates_count=self.total_stats['duplicates'],
                errors_count=self.total_stats['errors'],
                error_message=f"ì‹¤íŒ¨í•œ ì¹´í…Œê³ ë¦¬: {', '.join(self.total_stats['failed_categories'])}" if self.total_stats['failed_categories'] else None
            )

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì‹œì‚¬ì˜¤ëŠ˜ ì „ì²´ ì¹´í…Œê³ ë¦¬ ì™„ì „ í¬ë¡¤ë§')
    parser.add_argument('--workers', type=int, default=5, 
                       help='ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 5)')
    parser.add_argument('--max-pages', type=int, 
                       help='ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: ëª¨ë“  í˜ì´ì§€)')
    parser.add_argument('--delay', type=int, default=10, 
                       help='ì¹´í…Œê³ ë¦¬ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--skip', nargs='+', 
                       help='ê±´ë„ˆë›¸ ì¹´í…Œê³ ë¦¬ ëª©ë¡')
    parser.add_argument('--estimate-only', action='store_true', 
                       help='í˜ì´ì§€ ìˆ˜ë§Œ ì¶”ì •í•˜ê³  í¬ë¡¤ë§í•˜ì§€ ì•ŠìŒ')
    
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸš€ ì‹œì‚¬ì˜¤ëŠ˜ ì „ì²´ ì¹´í…Œê³ ë¦¬ ì™„ì „ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("="*80)
    
    # í¬ë¡¤ë§ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = FullCrawlingManager(
        max_workers=args.workers,
        delay_between_categories=args.delay
    )
    
    if args.estimate_only:
        # í˜ì´ì§€ ìˆ˜ë§Œ ì¶”ì •
        print("\nğŸ“Š í˜ì´ì§€ ìˆ˜ ì¶”ì • ëª¨ë“œ")
        category_pages = manager.estimate_total_pages()
        total_pages = sum(category_pages.values())
        print(f"\nì´ ì˜ˆìƒ í˜ì´ì§€ ìˆ˜: {total_pages}í˜ì´ì§€")
        return
    
    # ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
    success = manager.crawl_all_categories(
        max_pages_per_category=args.max_pages,
        skip_categories=args.skip
    )
    
    if success:
        print("\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nğŸ’¥ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

if __name__ == "__main__":
    main() 