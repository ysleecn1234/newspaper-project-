#!/usr/bin/env python3
"""
ì‹œì‚¬ì˜¤ëŠ˜ ë‰´ìŠ¤ ì„¹ì…˜ í¬ë¡¤ë§ ë° ê¸°ì ìˆœìœ„ ì‹œìŠ¤í…œ
ê°œì„ ëœ ë²„ì „ - ì•ˆì •ì„± ë° ì„±ëŠ¥ ìµœì í™”
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from database_manager import db_manager
from concurrent.futures import ThreadPoolExecutor, as_completed
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sisaon_crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SisaonCrawler:
    """ì‹œì‚¬ì˜¤ëŠ˜ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, max_workers: int = 3):
        self.base_url = "http://www.sisaon.co.kr"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        # ë‰´ìŠ¤ ì„¹ì…˜ ë©”ì¸ ì¹´í…Œê³ ë¦¬
        self.categories = {
            'ì •ì¹˜': 'S2N29',
            'ê²½ì œ': 'S2N30',
            'ì‚°ì—…': 'S2N31',
            'ê±´ì„¤Â·ë¶€ë™ì‚°': 'S2N32',
            'IT': 'S2N33',
            'ìœ í†µÂ·ë°”ì´ì˜¤': 'S2N34',
            'ì‚¬íšŒ': 'S2N35',
            'ìë™ì°¨': 'S2N55'
        }
        
        # í¬ë¡¤ë§ í†µê³„
        self.stats = {
            'total_articles': 0,
            'total_journalists': 0,
            'category_stats': {},
            'errors': 0,
            'duplicates': 0,
            'start_time': None,
            'end_time': None
        }
        
        # ì„±ëŠ¥ ì„¤ì •
        self.max_workers = max_workers
        self.request_delay = (1.0, 2.0)  # ëœë¤ ì§€ì—°
        self.timeout = 15
        self.max_retries = 3
        
        # ì„¸ì…˜ ê´€ë¦¬
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def _random_delay(self):
        """ëœë¤ ì§€ì—°ìœ¼ë¡œ ì„œë²„ ë¶€í•˜ ë°©ì§€"""
        delay = random.uniform(*self.request_delay)
        time.sleep(delay)
    
    def _fix_encoding_issues(self, text: str) -> str:
        """ì¸ì½”ë”© ë¬¸ì œ í•´ê²° (í•œê¸€ ê¹¨ì§ ìˆ˜ì •) - ê°œì„ ëœ ë²„ì „"""
        if not text:
            return text
        
        # 1. ì¼ë°˜ì ì¸ ê¹¨ì§„ ë¬¸ì íŒ¨í„´ ìˆ˜ì •
        encoding_fixes = {
            '': 'ê°€',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë‚˜',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë‹¤',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë¼',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë§ˆ',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë°”',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì‚¬',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì•„',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì°¨',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì¹´',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'íƒ€',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'íŒŒ',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'í•˜',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ê¸°',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë‹ˆ',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë””',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë¦¬',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë¯¸',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ë¹„',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì‹œ',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì´',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì§€',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'ì¹˜',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'í‚¤',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'í‹°',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'í”¼',  # ê¹¨ì§„ í•œê¸€ ììŒ
            '': 'íˆ',  # ê¹¨ì§„ í•œê¸€ ììŒ
        }
        
        # 2. ê¹¨ì§„ ë¬¸ì êµì²´
        for broken_char, fixed_char in encoding_fixes.items():
            text = text.replace(broken_char, fixed_char)
        
        # 3. ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ê¹¨ì§„ ë¬¸ì íŒ¨í„´ ìˆ˜ì •
        sisaon_fixes = {
            'íˆ': '',  # ì‹œì‚¬ì˜¤ëŠ˜ì—ì„œ ìì£¼ ë³´ì´ëŠ” íŒ¨í„´
            'íˆê´€íˆë ¨íˆê¸°íˆì‚¬íˆ': 'ê´€ë ¨ê¸°ì‚¬',
            'íˆì •íˆì¹˜íˆ': 'ì •ì¹˜',
            'íˆê²½íˆì œ': 'ê²½ì œ',
            'íˆì‚¬íˆíšŒ': 'ì‚¬íšŒ',
            'íˆìíˆë™íˆì°¨': 'ìë™ì°¨',
            'íˆìœ íˆí†µíˆë°”íˆì˜¤': 'ìœ í†µë°”ì´ì˜¤',
            'íˆê±´íˆì„¤íˆë¶€íˆë™íˆì‚°': 'ê±´ì„¤ë¶€ë™ì‚°',
            'íˆì‚°íˆì—…': 'ì‚°ì—…',
        }
        
        for broken_pattern, fixed_pattern in sisaon_fixes.items():
            text = text.replace(broken_pattern, fixed_pattern)
        
        # 4. ì¶”ê°€ ì¸ì½”ë”© íŒ¨í„´ ìˆ˜ì • (ìƒˆë¡œ ì¶”ê°€)
        additional_fixes = {
            'Ğ¼ Ğ¼Ğ½Ò³Ñ‘': 'ê¸°ìëª…',  # ê¹¨ì§„ ê¸°ìëª… íŒ¨í„´
            'ĞºÒ™Ò–Ğ» ĞĞºÑ‘Ğ¼Ó®': 'ê¸°ì‚¬ì œëª©',  # ê¹¨ì§„ ì œëª© íŒ¨í„´
            'Ğ»Ğ¼ Ò£Ğ¼Ò›Ò–': 'ê¸°ìëª…',  # ê¹¨ì§„ ê¸°ìëª… íŒ¨í„´
        }
        
        for broken_pattern, fixed_pattern in additional_fixes.items():
            text = text.replace(broken_pattern, fixed_pattern)
        
        # 5. ì—°ì†ëœ ê¹¨ì§„ ë¬¸ì ì œê±° (ë” ê°•í™”ëœ ë²„ì „)
        text = re.sub(r'[^\w\sê°€-í£.,!?()[\]{}"\'-]', '', text)
        
        # 6. ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        
        # 7. ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ ì œê±°
        if len(text.strip()) < 3:
            return ""
        
        # 8. í•œê¸€ì´ ì „í˜€ ì—†ëŠ” ê²½ìš° í•„í„°ë§
        if not re.search(r'[ê°€-í£]', text):
            return ""
        
        return text.strip()
    
    def _make_request(self, url: str, retries: int = None) -> Optional[requests.Response]:
        """ì•ˆì •ì ì¸ HTTP ìš”ì²­ (ì¸ì½”ë”© ê°•í™” ë²„ì „)"""
        if retries is None:
            retries = self.max_retries
        
        for attempt in range(retries + 1):
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                
                # ê°•í™”ëœ ì¸ì½”ë”© ì²˜ë¦¬
                try:
                    # 1. ì‘ë‹µ í—¤ë”ì—ì„œ ì¸ì½”ë”© í™•ì¸
                    if 'charset' in response.headers.get('content-type', '').lower():
                        content_type = response.headers['content-type'].lower()
                        if 'charset=utf-8' in content_type:
                            response.encoding = 'utf-8'
                        elif 'charset=euc-kr' in content_type:
                            response.encoding = 'euc-kr'
                        elif 'charset=cp949' in content_type:
                            response.encoding = 'cp949'
                    
                    # 2. HTML ë©”íƒ€ íƒœê·¸ì—ì„œ ì¸ì½”ë”© í™•ì¸
                    if response.encoding in ['ISO-8859-1', 'ascii']:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        meta_charset = soup.find('meta', charset=True)
                        if meta_charset:
                            response.encoding = meta_charset['charset']
                        else:
                            meta_content = soup.find('meta', attrs={'http-equiv': 'Content-Type'})
                            if meta_content and 'charset=' in meta_content.get('content', ''):
                                charset = meta_content['content'].split('charset=')[-1].strip()
                                response.encoding = charset
                    
                    # 3. ì¼ë°˜ì ì¸ í•œê¸€ ì‚¬ì´íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬
                    if response.encoding in ['ISO-8859-1', 'ascii', 'cp949']:
                        # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        content_text = response.text
                        if any(ord(char) > 127 for char in content_text[:1000]):
                            # í•œê¸€ì´ ìˆìœ¼ë©´ UTF-8ë¡œ ê°•ì œ ì„¤ì •
                            response.encoding = 'utf-8'
                    
                    # 4. ìµœì¢… ê²€ì¦ - í•œê¸€ ê¹¨ì§ í™•ì¸
                    test_text = response.text[:200]
                    if '' in test_text or '?' in test_text:
                        # ê¹¨ì§„ ë¬¸ìê°€ ìˆìœ¼ë©´ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
                        for encoding in ['utf-8', 'euc-kr', 'cp949']:
                            try:
                                test_content = response.content.decode(encoding)
                                if '' not in test_content[:200] and '?' not in test_content[:200]:
                                    response.encoding = encoding
                                    break
                            except UnicodeDecodeError:
                                continue
                    
                    # 5. ì‹œì‚¬ì˜¤ëŠ˜ ì‚¬ì´íŠ¸ íŠ¹í™” ì¸ì½”ë”© ì²˜ë¦¬
                    if 'sisaon.co.kr' in url:
                        # ì‹œì‚¬ì˜¤ëŠ˜ì€ ë³´í†µ UTF-8ì„ ì‚¬ìš©í•˜ì§€ë§Œ ê°€ë” ê¹¨ì§
                        if response.encoding not in ['utf-8', 'euc-kr']:
                            response.encoding = 'utf-8'
                
                except Exception as encoding_error:
                    logger.warning(f"ì¸ì½”ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {encoding_error}")
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ UTF-8 ì„¤ì •
                    response.encoding = 'utf-8'
                
                return response
            except requests.exceptions.RequestException as e:
                if attempt == retries:
                    logger.error(f"ìš”ì²­ ì‹¤íŒ¨ (ìµœì¢…): {url} - {e}")
                    return None
                else:
                    logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}/{retries}): {url} - {e}")
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return None
    
    def get_article_links_from_page(self, category_url: str) -> List[str]:
        """í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ê°œì„  ë²„ì „)"""
        try:
            response = self._make_request(category_url)
            if not response:
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ê¸°ì‚¬ ë§í¬ íŒ¨í„´
            link_patterns = [
                r'/news/articleView\.html\?idxno=\d+',
                r'/news/articleView\.html\?idxno=\d+&.*',
                r'/articleView\.html\?idxno=\d+',
                r'/news/view\.html\?idxno=\d+',
                r'/article/view\.html\?idxno=\d+'
            ]
            
            article_links = []
            seen_links = set()
            
            # ë°©ë²• 1: ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ë§í¬ ì°¾ê¸°
            for pattern in link_patterns:
                links = soup.find_all('a', href=re.compile(pattern))
                for link in links:
                    href = link.get('href')
                    if href and href not in seen_links:
                        # URL ì •ê·œí™”
                        if href.startswith('/'):
                            full_url = self.base_url + href
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        
                        # URL íŒŒë¼ë¯¸í„° ì •ë¦¬ (idxnoë§Œ ìœ ì§€)
                        if '?' in full_url:
                            base_url = full_url.split('?')[0]
                            params = full_url.split('?')[1]
                            idxno_match = re.search(r'idxno=(\d+)', params)
                            if idxno_match:
                                full_url = f"{base_url}?idxno={idxno_match.group(1)}"
                        
                        article_links.append(full_url)
                        seen_links.add(href)
            
            # ë°©ë²• 2: ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” í´ë˜ìŠ¤ë¡œ ë§í¬ ì°¾ê¸°
            specific_selectors = [
                '.article-list a',
                '.news-list a', 
                '.list-article a',
                '.article-item a',
                '.news-item a'
            ]
            
            for selector in specific_selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href and href not in seen_links:
                        # ê¸°ì‚¬ ë§í¬ì¸ì§€ í™•ì¸
                        if any(pattern in href for pattern in ['articleView', 'view.html']):
                            if href.startswith('/'):
                                full_url = self.base_url + href
                            elif href.startswith('http'):
                                full_url = href
                            else:
                                continue
                            
                            # URL íŒŒë¼ë¯¸í„° ì •ë¦¬
                            if '?' in full_url:
                                base_url = full_url.split('?')[0]
                                params = full_url.split('?')[1]
                                idxno_match = re.search(r'idxno=(\d+)', params)
                                if idxno_match:
                                    full_url = f"{base_url}?idxno={idxno_match.group(1)}"
                            
                            article_links.append(full_url)
                            seen_links.add(href)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_links = list(set(article_links))
            unique_links.sort()
            
            logger.info(f"í˜ì´ì§€ì—ì„œ {len(unique_links)}ê°œ ê¸°ì‚¬ ë§í¬ ë°œê²¬")
            return unique_links
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_total_pages(self, category_url: str) -> int:
        """ì¹´í…Œê³ ë¦¬ì˜ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        try:
            response = self._make_request(category_url)
            if not response:
                return 1
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë°©ë²• 1: í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ì—ì„œ ì°¾ê¸°
            pagination_links = soup.find_all('a', href=re.compile(r'page=\d+'))
            
            if pagination_links:
                page_numbers = []
                for link in pagination_links:
                    href = link.get('href', '')
                    page_match = re.search(r'page=(\d+)', href)
                    if page_match:
                        page_numbers.append(int(page_match.group(1)))
                
                if page_numbers:
                    max_page = max(page_numbers)
                    logger.info(f"í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ì´ {max_page}í˜ì´ì§€ ë°œê²¬")
                    return max_page
            
            # ë°©ë²• 2: í˜ì´ì§€ ë²ˆí˜¸ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
            page_texts = soup.find_all(text=re.compile(r'\d+'))
            for text in page_texts:
                if 'í˜ì´ì§€' in text or 'page' in text.lower():
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        max_page = max(int(num) for num in numbers)
                        logger.info(f"í…ìŠ¤íŠ¸ì—ì„œ ì´ {max_page}í˜ì´ì§€ ë°œê²¬")
                        return max_page
            
            # ë°©ë²• 3: "ë‹¤ìŒ" ë²„íŠ¼ì´ë‚˜ "ë§ˆì§€ë§‰" ë²„íŠ¼ ì°¾ê¸°
            next_links = soup.find_all('a', string=re.compile(r'ë‹¤ìŒ|ë§ˆì§€ë§‰|>>|>'))
            if next_links:
                for link in next_links:
                    href = link.get('href', '')
                    page_match = re.search(r'page=(\d+)', href)
                    if page_match:
                        max_page = int(page_match.group(1))
                        logger.info(f"ë‹¤ìŒ/ë§ˆì§€ë§‰ ë²„íŠ¼ì—ì„œ ì´ {max_page}í˜ì´ì§€ ë°œê²¬")
                        return max_page
            
            logger.info("í˜ì´ì§€ ìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ 1í˜ì´ì§€ë¡œ ì„¤ì •")
            return 1
                
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
            return 1
    
    def extract_article_data(self, article_url: str, category: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            response = self._make_request(article_url)
            if not response:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)
            title = self._extract_title(soup)
            if not title:
                logger.warning(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {article_url}")
                return None
            
            # ê¸°ì ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)
            author = self._extract_author(soup)
            if not author:
                logger.warning(f"ê¸°ì ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {article_url}")
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)
            content = self._extract_content(soup)
            if not content:
                logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {article_url}")
                return None
            
            # ë°œí–‰ì¼ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)
            published_date = self._extract_published_date(soup)
            
            return {
                'title': title,
                'content': content,
                'url': article_url,
                'source': 'ì‹œì‚¬ì˜¤ëŠ˜',
                'author': author,
                'published_date': published_date,
                'categories': [category],
                'tags': [],
                'metadata': {
                    'crawled_at': datetime.now().isoformat(),
                    'category': category,
                    'word_count': len(content.split())
                }
            }
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {article_url}: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª© ì¶”ì¶œ (ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ê°•í™” ë²„ì „)"""
        # ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ì œëª© ì„ íƒì (ìš°ì„ ìˆœìœ„ ìˆœ)
        title_selectors = [
            '.aht-title',
            '.article-head-title', 
            '.aht-title-view',
            '.article-title',
            '.news-title',
            '.title',
            'h1',
            'h2',
            'h3',
            '[class*="title"]',
            '[class*="headline"]'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                
                # ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                title = self._fix_encoding_issues(title)
                
                # ì œëª© ìœ íš¨ì„± ê²€ì‚¬
                if (title and 
                    len(title) > 5 and 
                    len(title) < 200 and  # ë„ˆë¬´ ê¸´ ì œëª© ì œì™¸
                    '[' not in title and  # ê´‘ê³ ì„± ì œëª© ì œì™¸
                    'http' not in title.lower() and  # URL ì œì™¸
                    not title.startswith('ê´‘ê³ ') and  # ê´‘ê³  ì œì™¸
                    not title.startswith('PR') and   # PR ì œì™¸
                    not re.match(r'^\d+$', title)):  # ìˆ«ìë§Œ ìˆëŠ” ì œëª© ì œì™¸
                    return title
        
        # ë°©ë²• 2: ë©”íƒ€ íƒœê·¸ì—ì„œ ì œëª© ì°¾ê¸°
        meta_title = soup.find('meta', property='og:title')
        if meta_title:
            title = meta_title.get('content', '').strip()
            if title and len(title) > 5 and len(title) < 200:
                return title
        
        # ë°©ë²• 3: title íƒœê·¸ì—ì„œ ì°¾ê¸°
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            if title and len(title) > 5 and len(title) < 200:
                # ë¶ˆí•„ìš”í•œ ì ‘ë¯¸ì‚¬ ì œê±°
                title = re.sub(r'\s*[-|]\s*ì‹œì‚¬ì˜¤ëŠ˜.*$', '', title)
                title = re.sub(r'\s*[-|]\s*ì‹œì‚¬ON.*$', '', title)
                if len(title) > 5:
                    return title
        
        # ë°©ë²• 4: ë” ë„“ì€ ë²”ìœ„ì—ì„œ ì œëª© ì°¾ê¸°
        for elem in soup.find_all(['h1', 'h2', 'h3', 'h4']):
            title = elem.get_text().strip()
            if (title and 
                len(title) > 5 and 
                len(title) < 200 and
                not title.startswith('ê´‘ê³ ') and
                not title.startswith('PR')):
                return title
        
        return None
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì ì •ë³´ ì¶”ì¶œ (ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ê°œì„  ë²„ì „)"""
        # ë°©ë²• 1: ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” - ê¸°ì í”„ë¡œí•„ ì„¹ì…˜
        profile_selectors = [
            '#wrProfile .name strong',
            '.writer-info .name',
            '.author-info .name',
            '.reporter-info .name'
        ]
        
        for selector in profile_selectors:
            profile_name = soup.select_one(selector)
            if profile_name:
                author = profile_name.get_text().strip()
                # ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                author = self._fix_encoding_issues(author)
                if author and len(author) <= 10 and 'ê¸°ì' not in author:
                    return author
        
        # ë°©ë²• 2: ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
        meta_author = soup.find('meta', property='og:article:author')
        if meta_author:
            author = meta_author.get('content', '').strip()
            if author and 'ê¸°ì' in author:
                return author.split('ê¸°ì')[0].strip()
        
        # ë°©ë²• 3: twitter creator ë©”íƒ€ íƒœê·¸
        twitter_author = soup.find('meta', attrs={'name': 'twitter:creator'})
        if twitter_author:
            author = twitter_author.get('content', '').strip()
            if author and 'ê¸°ì' in author:
                return author.split('ê¸°ì')[0].strip()
        
        # ë°©ë²• 4: dable author ë©”íƒ€ íƒœê·¸
        dable_author = soup.find('meta', property='dable:author')
        if dable_author:
            author = dable_author.get('content', '').strip()
            if author and 'ê¸°ì' in author:
                return author.split('ê¸°ì')[0].strip()
        
        # ë°©ë²• 5: ê¸°ì ì •ë³´ ì„¹ì…˜ì—ì„œ ì¶”ì¶œ (ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™”)
        info_selectors = [
            '.info-text li',
            '.article-info .reporter',
            '.article-meta .author',
            '.byline'
        ]
        
        for selector in info_selectors:
            info_text = soup.select_one(selector)
            if info_text:
                info_content = info_text.get_text().strip()
                if 'ê¸°ì' in info_content:
                    # ë‹¤ì–‘í•œ íŒ¨í„´ ë§¤ì¹­
                    patterns = [
                        r'([ê°€-í£]{2,4})\s*ê¸°ì',
                        r'ê¸°ì\s*([ê°€-í£]{2,4})',
                        r'([ê°€-í£]{2,4})\s*ê¸°ì\s*[ê°€-í£]*',
                        r'=\s*([ê°€-í£]+)\s*ê¸°ì'
                    ]
                    
                    for pattern in patterns:
                        author_match = re.search(pattern, info_content)
                        if author_match:
                            author = author_match.group(1).strip()
                            if len(author) >= 2 and len(author) <= 4:
                                return author
        
        # ë°©ë²• 6: ë³¸ë¬¸ ì²« ë¶€ë¶„ì—ì„œ ì¶”ì¶œ (ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™”)
        p_elements = soup.find_all('p')
        for p_elem in p_elements[:5]:  # ì²˜ìŒ 5ê°œ ë¬¸ë‹¨ë§Œ í™•ì¸
            p_text = p_elem.get_text().strip()
            if 'ê¸°ì' in p_text and ('=' in p_text or 'Â·' in p_text):
                # ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” íŒ¨í„´
                patterns = [
                    r'=\s*([ê°€-í£]+)\s*ê¸°ì',
                    r'Â·\s*([ê°€-í£]+)\s*ê¸°ì',
                    r'([ê°€-í£]{2,4})\s*ê¸°ì',
                    r'ê¸°ì\s*([ê°€-í£]{2,4})'
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, p_text)
                    if match:
                        author = match.group(1).strip()
                        if len(author) >= 2 and len(author) <= 4:
                            return author
        
        # ë°©ë²• 7: ì œëª© ê·¼ì²˜ì—ì„œ ê¸°ì ì •ë³´ ì°¾ê¸°
        title_area = soup.find('h1') or soup.find('h2') or soup.find('.title')
        if title_area:
            title_parent = title_area.parent
            if title_parent:
                for elem in title_parent.find_all(['span', 'div', 'p']):
                    text = elem.get_text().strip()
                    if 'ê¸°ì' in text:
                        author_match = re.search(r'([ê°€-í£]{2,4})\s*ê¸°ì', text)
                        if author_match:
                            return author_match.group(1).strip()
        
        return None
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """ë³¸ë¬¸ ì¶”ì¶œ (ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ê°œì„  ë²„ì „)"""
        # ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ì½˜í…ì¸  ì„ íƒì
        content_selectors = [
            'article.article-veiw-body',
            '.user-content',
            '.article-content',
            '.content-body',
            '.article-body',
            '.news-content',
            '.content',
            '.body',
            'article',
            '.article'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                unwanted_selectors = [
                    '.related-articles', '.comments', '.advertisement', 
                    '.ad', '.banner', '.social-share', '.article-footer',
                    'script', 'style', '.recommend', '.news-recommend',
                    '.article-recommend', '.related-news', '.more-news'
                ]
                
                for unwanted_selector in unwanted_selectors:
                    for unwanted in content_elem.select(unwanted_selector):
                        unwanted.decompose()
                
                # ê¸°ì ì •ë³´ê°€ í¬í•¨ëœ ì²« ë¬¸ë‹¨ ì œê±°
                p_elements = content_elem.find_all('p')
                if p_elements:
                    first_p = p_elements[0]
                    first_text = first_p.get_text().strip()
                    if 'ê¸°ì' in first_text and ('=' in first_text or 'Â·' in first_text):
                        first_p.decompose()
                
                content_text = content_elem.get_text().strip()
                if content_text and len(content_text) > 100:
                    # ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                    content_text = self._fix_encoding_issues(content_text)
                    # ê´€ë ¨ê¸°ì‚¬ë‚˜ ëŒ“ê¸€ ë¶€ë¶„ ì œê±°
                    if 'ê´€ë ¨ê¸°ì‚¬' not in content_text and 'ëŒ“ê¸€' not in content_text:
                        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
                        content_text = re.sub(r'\s+', ' ', content_text)
                        content_text = re.sub(r'\n\s*\n', '\n', content_text)
                        return content_text.strip()
        
        return None
    
    def _extract_published_date(self, soup: BeautifulSoup) -> datetime:
        """ë°œí–‰ì¼ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)"""
        try:
            # ë©”íƒ€ íƒœê·¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
            date_selectors = [
                'meta[property="og:published_time"]',
                'meta[name="publish_date"]',
                'meta[name="article:published_time"]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_str = date_elem.get('content', '')
                    if date_str:
                        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            
            # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´
            return datetime.now()
            
        except Exception:
            return datetime.now()
    
    def crawl_category(self, category: str, category_code: str, max_pages: int = None) -> int:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (ì‹œì‚¬ì˜¤ëŠ˜ íŠ¹í™” ê°œì„  ë²„ì „)"""
        logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì‹œì‘...")
        
        category_url = f"{self.base_url}/news/articleList.html?sc_sub_section_code={category_code}&view_type=sm"
        
        # ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸
        total_pages = self.get_total_pages(category_url)
        if max_pages:
            total_pages = min(total_pages, max_pages)
        
        logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}': ì´ {total_pages}í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì •")
        
        category_articles = 0
        
        for page in range(1, total_pages + 1):
            try:
                page_url = f"{category_url}&page={page}"
                logger.info(f"í˜ì´ì§€ {page}/{total_pages} í¬ë¡¤ë§ ì¤‘... ({page/total_pages*100:.1f}%)")
                
                # í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                article_links = self.get_article_links_from_page(page_url)
                
                if not article_links:
                    logger.warning(f"í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                logger.info(f"í˜ì´ì§€ {page}ì—ì„œ {len(article_links)}ê°œ ê¸°ì‚¬ ë§í¬ ë°œê²¬")
                
                # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ í¬ë¡¤ë§ (ì•ˆì •ì„± ê°œì„ )
                successful_articles = 0
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # ì‘ì—… ì œì¶œ
                    future_to_url = {
                        executor.submit(self._process_article, article_url, category): article_url 
                        for article_url in article_links
                    }
                    
                    # ê²°ê³¼ ìˆ˜ì§‘
                    for future in as_completed(future_to_url):
                        article_url = future_to_url[future]
                        try:
                            result = future.result(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
                            if result:
                                successful_articles += 1
                                category_articles += 1
                                self.stats['total_articles'] += 1
                        except Exception as e:
                            logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ {article_url}: {e}")
                            self.stats['errors'] += 1
                
                logger.info(f"í˜ì´ì§€ {page} ì™„ë£Œ: {successful_articles}/{len(article_links)} ê¸°ì‚¬ ì„±ê³µ")
                
                # í˜ì´ì§€ ê°„ê²© ì¡°ì ˆ
                self._random_delay()
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                self.stats['errors'] += 1
                continue
        
        self.stats['category_stats'][category] = category_articles
        logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì™„ë£Œ: {category_articles}ê°œ ê¸°ì‚¬")
        
        return category_articles
    
    def _process_article(self, article_url: str, category: str) -> bool:
        """ê°œë³„ ê¸°ì‚¬ ì²˜ë¦¬ (journalists í…Œì´ë¸”ì—ë§Œ ì €ì¥)"""
        try:
            # ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ (ì¬ì‹œë„ í¬í•¨)
            article_data = None
            for retry in range(3):
                try:
                    article_data = self.extract_article_data(article_url, category)
                    if article_data:
                        break
                    time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                except Exception as retry_e:
                    logger.warning(f"ê¸°ì‚¬ ì¶”ì¶œ ì¬ì‹œë„ {retry+1}/3: {retry_e}")
                    if retry < 2:
                        time.sleep(3)
            
            if article_data and article_data.get('author'):
                # journalists í…Œì´ë¸”ì— ê¸°ì í†µê³„ì™€ ê¸°ì‚¬ ë‚´ìš© ì—…ë°ì´íŠ¸
                saved = False
                if hasattr(db_manager, 'connection_pool') and db_manager.connection_pool:
                    try:
                        if db_manager.update_journalist_stats(
                            journalist_name=article_data['author'],
                            category=category,
                            increment=1,
                            article_data=article_data
                        ):
                            saved = True
                    except Exception as e:
                        logger.warning(f"ê¸°ì í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                if saved:
                    logger.info(f"ê¸°ì í†µê³„ ë° ê¸°ì‚¬ ë‚´ìš© ì €ì¥ ì™„ë£Œ: {article_data['title'][:50]}... (ê¸°ì: {article_data.get('author', 'N/A')})")
                else:
                    logger.info(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ: {article_data['title'][:50]}... (ê¸°ì: {article_data.get('author', 'N/A')}) - ì €ì¥ ì•ˆë¨")
                
                return True
            else:
                if not article_data:
                    self.stats['errors'] += 1
                    logger.warning(f"ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {article_url}")
                else:
                    logger.warning(f"ê¸°ì ì •ë³´ ì—†ìŒ: {article_url}")
                return False
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ {article_url}: {e}")
            self.stats['errors'] += 1
            return False
    
    def crawl_all_categories(self, max_pages_per_category: int = None):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (ê°œì„ ëœ ë²„ì „)"""
        logger.info("ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘...")
        
        self.stats['start_time'] = datetime.now()
        
        for category, code in self.categories.items():
            try:
                self.crawl_category(category, code, max_pages_per_category)
                
                # ì¹´í…Œê³ ë¦¬ ê°„ê²© ì¡°ì ˆ
                time.sleep(3)
                
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                self.stats['errors'] += 1
                continue
        
        self.stats['end_time'] = datetime.now()
        self._print_final_stats()
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        logger.info(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ì´ ê¸°ì‚¬ ìˆ˜: {self.stats['total_articles']}ê°œ")
        logger.info(f"ì¤‘ë³µ ê¸°ì‚¬ ìˆ˜: {self.stats['duplicates']}ê°œ")
        logger.info(f"ì´ ì˜¤ë¥˜ ìˆ˜: {self.stats['errors']}ê°œ")
        logger.info(f"ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì¶œë ¥
        print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ ê²°ê³¼:")
        print("-" * 50)
        for category, count in self.stats['category_stats'].items():
            print(f"{category:<15}: {count:4d}ê°œ ê¸°ì‚¬")
        
        # ì„±ê³µë¥  ê³„ì‚°
        total_attempts = self.stats['total_articles'] + self.stats['errors'] + self.stats['duplicates']
        if total_attempts > 0:
            success_rate = (self.stats['total_articles'] / total_attempts) * 100
            print(f"\nâœ… ì„±ê³µë¥ : {success_rate:.1f}%")
        
        print(f"â±ï¸  í‰ê·  ì²˜ë¦¬ ì†ë„: {self.stats['total_articles']/duration*60:.1f}ê°œ/ë¶„")
        
        # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
        if hasattr(db_manager, 'connection_pool') and db_manager.connection_pool:
            db_manager.log_crawling_job(
                job_type='sisaon_news_crawling',
                source_key='ì‹œì‚¬ì˜¤ëŠ˜',
                status='completed',
                articles_count=self.stats['total_articles'],
                duration_seconds=duration,
                error_message=f"ì˜¤ë¥˜ {self.stats['errors']}ê°œ, ì¤‘ë³µ {self.stats['duplicates']}ê°œ" if self.stats['errors'] > 0 or self.stats['duplicates'] > 0 else None
            )

class JournalistRankingSystem:
    """ê¸°ì ìˆœìœ„ ì‹œìŠ¤í…œ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self):
        self.categories = {
            'ì •ì¹˜': 'S2N29',
            'ê²½ì œ': 'S2N30',
            'ì‚°ì—…': 'S2N31',
            'ê±´ì„¤Â·ë¶€ë™ì‚°': 'S2N32',
            'IT': 'S2N33',
            'ìœ í†µÂ·ë°”ì´ì˜¤': 'S2N34',
            'ì‚¬íšŒ': 'S2N35',
            'ìë™ì°¨': 'S2N55'
        }
        
        # í†µê³„ ìºì‹œ
        self._stats_cache = {}
        self._cache_expiry = {}
        self.cache_duration = 300  # 5ë¶„
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """ìºì‹œ ìœ íš¨ì„± í™•ì¸"""
        if cache_key not in self._cache_expiry:
            return False
        return datetime.now() < self._cache_expiry[cache_key]
    
    def _set_cache(self, cache_key: str, data: Any):
        """ìºì‹œ ì„¤ì •"""
        self._stats_cache[cache_key] = data
        self._cache_expiry[cache_key] = datetime.now() + timedelta(seconds=self.cache_duration)
    
    def _get_cache(self, cache_key: str) -> Any:
        """ìºì‹œ ì¡°íšŒ"""
        if self._is_cache_valid(cache_key):
            return self._stats_cache.get(cache_key)
        return None
    
    def generate_journalist_stats(self, force_refresh: bool = False) -> bool:
        """ê¸°ì í†µê³„ ìƒì„± (journalists í…Œì´ë¸” ê¸°ë°˜)"""
        logger.info("ê¸°ì í†µê³„ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        cache_key = "journalist_stats"
        if not force_refresh:
            cached_data = self._get_cache(cache_key)
            if cached_data:
                logger.info("ìºì‹œëœ ê¸°ì í†µê³„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return True
        
        try:
            # journalists í…Œì´ë¸”ì—ì„œ ê¸°ì í†µê³„ ì¡°íšŒ
            all_journalists = db_manager.get_all_journalists(source='ì‹œì‚¬ì˜¤ëŠ˜', limit=1000)
            
            if all_journalists:
                # ê¸°ìë³„ ì´ ê¸°ì‚¬ ìˆ˜ ë° ì¹´í…Œê³ ë¦¬ ë¶„í¬ ê³„ì‚°
                journalist_totals = {}
                journalist_categories = {}
                
                for journalist in all_journalists:
                    name = journalist['name']
                    total_articles = journalist['total_articles']
                    categories = journalist.get('categories', [])
                    
                    # ì´ ê¸°ì‚¬ ìˆ˜ ì €ì¥
                    journalist_totals[name] = total_articles
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ëŠ” news_articlesì—ì„œ ê³„ì‚°
                    journalist_stats = db_manager.get_journalist_stats_by_journalist(name)
                    if journalist_stats:
                        journalist_categories[name] = {
                            stat['category']: stat['article_count'] 
                            for stat in journalist_stats
                        }
                    else:
                        journalist_categories[name] = {}
                
                # ì´ ê¸°ì‚¬ ìˆ˜ë¡œ ì •ë ¬
                sorted_journalists = sorted(journalist_totals.items(), key=lambda x: x[1], reverse=True)
                
                # ìºì‹œì— ì €ì¥
                stats_data = {
                    'journalist_totals': journalist_totals,
                    'journalist_categories': journalist_categories,
                    'sorted_journalists': sorted_journalists,
                    'total_journalists': len(sorted_journalists),
                    'total_articles': sum(journalist_totals.values())
                }
                self._set_cache(cache_key, stats_data)
                
                logger.info(f"ê¸°ì í†µê³„ ìƒì„± ì™„ë£Œ:")
                logger.info(f"  - ì´ ê¸°ì ìˆ˜: {len(sorted_journalists)}ëª…")
                logger.info(f"  - ì´ ê¸°ì‚¬ ìˆ˜: {sum(journalist_totals.values())}ê°œ")
                
                # ê¸°ìë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥
                self._print_journalist_summary(sorted_journalists)
                
                return True
            else:
                logger.warning("ì‹œì‚¬ì˜¤ëŠ˜ ê¸°ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            logger.error(f"ê¸°ì í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def _print_journalist_summary(self, sorted_journalists: List[Tuple[str, int]]):
        """ê¸°ì ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print(f"\nğŸ“Š ì‹œì‚¬ì˜¤ëŠ˜ ê¸°ì í˜„í™©:")
        print("-" * 60)
        print(f"{'ìˆœìœ„':<4} {'ê¸°ìëª…':<15} {'ì´ ê¸°ì‚¬ìˆ˜':<10} {'ì£¼ìš” ì¹´í…Œê³ ë¦¬':<20}")
        print("-" * 60)
        
        for i, (name, total) in enumerate(sorted_journalists[:15], 1):  # ìƒìœ„ 15ëª… ì¶œë ¥
            # ì£¼ìš” ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
            categories = self._get_cache("journalist_stats")
            if categories and 'journalist_categories' in categories:
                journalist_cats = categories['journalist_categories'].get(name, {})
                if journalist_cats:
                    top_category = max(journalist_cats.items(), key=lambda x: x[1])
                    main_category = f"{top_category[0]}({top_category[1]})"
                else:
                    main_category = "N/A"
            else:
                main_category = "N/A"
            
            # ìƒìœ„ 3ëª…ì€ íŠ¹ë³„ í‘œì‹œ
            if i == 1:
                print(f"ğŸ¥‡ {i:<2} {name:<15} {total:<10} {main_category}")
            elif i == 2:
                print(f"ğŸ¥ˆ {i:<2} {name:<15} {total:<10} {main_category}")
            elif i == 3:
                print(f"ğŸ¥‰ {i:<2} {name:<15} {total:<10} {main_category}")
            else:
                print(f"   {i:<2} {name:<15} {total:<10} {main_category}")
        
        if len(sorted_journalists) > 15:
            print(f"... ì™¸ {len(sorted_journalists) - 15}ëª…")
    
    def get_journalist_rankings_by_category(self, category: str, limit: int = 20) -> list:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ê¸°ì ìˆœìœ„ ì¡°íšŒ (journalists í…Œì´ë¸” ê¸°ë°˜)"""
        cache_key = f"category_rankings_{category}_{limit}"
        cached_data = self._get_cache(cache_key)
        if cached_data:
            return cached_data
        
        try:
            # journalists í…Œì´ë¸”ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ê¸°ì‚¬ë¥¼ ì“´ ê¸°ìë“¤ ì¡°íšŒ
            all_journalists = db_manager.get_all_journalists(source='ì‹œì‚¬ì˜¤ëŠ˜', limit=1000)
            
            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ê¸°ì‚¬ë¥¼ ì“´ ê¸°ìë“¤ í•„í„°ë§
            category_journalists = []
            for journalist in all_journalists:
                categories = journalist.get('categories', [])
                if category in categories:
                    category_journalists.append({
                        'journalist_name': journalist['name'],
                        'article_count': journalist['total_articles'],
                        'category': category,
                        'updated_at': journalist['updated_at'],
                        'total_articles': journalist['total_articles'],
                        'category_count': len(categories)
                    })
            
            # ê¸°ì‚¬ ìˆ˜ë¡œ ì •ë ¬
            category_journalists.sort(key=lambda x: x['article_count'], reverse=True)
            
            # ìˆœìœ„ ì¶”ê°€
            rankings = []
            for i, ranking in enumerate(category_journalists[:limit], 1):
                ranking['rank'] = i
                rankings.append(ranking)
            
            self._set_cache(cache_key, rankings)
            return rankings
            
        except Exception as e:
            logger.error(f"{category} ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def print_category_rankings(self, category: str, limit: int = 10):
        """ì¹´í…Œê³ ë¦¬ë³„ ìˆœìœ„ ì¶œë ¥ (ê°œì„ ëœ ë²„ì „)"""
        try:
            category_rankings = self.get_journalist_rankings_by_category(category, limit)
            
            if category_rankings:
                print(f"\nğŸ† {category} ì¹´í…Œê³ ë¦¬ ê¸°ì ìˆœìœ„ (ìƒìœ„ {len(category_rankings)}ëª…)")
                print("=" * 80)
                print(f"{'ìˆœìœ„':<4} {'ê¸°ìëª…':<15} {'ê¸°ì‚¬ìˆ˜':<8} {'ì „ì²´ê¸°ì‚¬':<10} {'ì¹´í…Œê³ ë¦¬ìˆ˜':<10} {'ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸':<20}")
                print("-" * 80)
                
                for ranking in category_rankings:
                    name = ranking['journalist_name'][:14]
                    count = ranking['article_count']
                    total = ranking.get('total_articles', 0)
                    cat_count = ranking.get('category_count', 0)
                    updated = ranking.get('updated_at', 'N/A')
                    if updated != 'N/A':
                        updated = updated.strftime('%Y-%m-%d %H:%M')
                    
                    rank = ranking['rank']
                    # ìƒìœ„ 3ëª…ì€ íŠ¹ë³„ í‘œì‹œ
                    if rank == 1:
                        print(f"ğŸ¥‡ {rank:<2} {name:<15} {count:<8} {total:<10} {cat_count:<10} {updated}")
                    elif rank == 2:
                        print(f"ğŸ¥ˆ {rank:<2} {name:<15} {count:<8} {total:<10} {cat_count:<10} {updated}")
                    elif rank == 3:
                        print(f"ğŸ¥‰ {rank:<2} {name:<15} {count:<8} {total:<10} {cat_count:<10} {updated}")
                    else:
                        print(f"   {rank:<2} {name:<15} {count:<8} {total:<10} {cat_count:<10} {updated}")
            else:
                print(f"\nâŒ {category} ì¹´í…Œê³ ë¦¬ì— ê¸°ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"{category} ìˆœìœ„ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    def print_all_rankings(self, limit: int = 10):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ì¶œë ¥ (ê°œì„ ëœ ë²„ì „)"""
        try:
            print("\n" + "="*100)
            print("ğŸ“Š ì „ì²´ ì¹´í…Œê³ ë¦¬ ê¸°ì ìˆœìœ„ í˜„í™©")
            print("="*100)
            
            # ì „ì²´ ìˆœìœ„ ì¡°íšŒ (journalists í…Œì´ë¸” ê¸°ë°˜)
            all_journalists = db_manager.get_all_journalists(source='ì‹œì‚¬ì˜¤ëŠ˜', limit=limit)
            if all_journalists:
                print("\nğŸ† ì „ì²´ ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ ìƒìœ„ ê¸°ì")
                print("-" * 90)
                print(f"{'ìˆœìœ„':<4} {'ê¸°ìëª…':<20} {'ì´ ê¸°ì‚¬ìˆ˜':<10} {'ì¹´í…Œê³ ë¦¬ ìˆ˜':<12} {'ì£¼ìš” ì¹´í…Œê³ ë¦¬':<20} {'ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸':<20}")
                print("-" * 90)
                
                for i, journalist in enumerate(all_journalists[:10], 1):
                    name = journalist['name'][:19]
                    total = journalist['total_articles']
                    categories = len(journalist.get('categories', []))
                    
                    # ì£¼ìš” ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
                    categories_list = journalist.get('categories', [])
                    if categories_list:
                        main_category = f"{categories_list[0]}({total})"
                    else:
                        main_category = "N/A"
                    
                    updated = journalist.get('updated_at', 'N/A')
                    if updated != 'N/A':
                        updated = updated.strftime('%Y-%m-%d %H:%M')
                    
                    # ìƒìœ„ 3ëª…ì€ íŠ¹ë³„ í‘œì‹œ
                    if i == 1:
                        print(f"ğŸ¥‡ {i:<2} {name:<20} {total:<10} {categories:<12} {main_category:<20} {updated}")
                    elif i == 2:
                        print(f"ğŸ¥ˆ {i:<2} {name:<20} {total:<10} {categories:<12} {main_category:<20} {updated}")
                    elif i == 3:
                        print(f"ğŸ¥‰ {i:<2} {name:<20} {total:<10} {categories:<12} {main_category:<20} {updated}")
                    else:
                        print(f"   {i:<2} {name:<20} {total:<10} {categories:<12} {main_category:<20} {updated}")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìˆœìœ„ ìš”ì•½
            print(f"\nğŸ“° ì¹´í…Œê³ ë¦¬ë³„ TOP 3 ìš”ì•½:")
            print("-" * 80)
            for category in self.categories.keys():
                category_rankings = self.get_journalist_rankings_by_category(category, limit=3)
                
                if category_rankings:
                    top_3 = []
                    for ranking in category_rankings[:3]:
                        name = ranking['journalist_name'][:12]
                        count = ranking['article_count']
                        rank = ranking['rank']
                        
                        if rank == 1:
                            top_3.append(f"ğŸ¥‡{name}({count})")
                        elif rank == 2:
                            top_3.append(f"ğŸ¥ˆ{name}({count})")
                        elif rank == 3:
                            top_3.append(f"ğŸ¥‰{name}({count})")
                    
                    print(f"{category:<15}: {' | '.join(top_3)}")
                else:
                    print(f"{category:<15}: ë°ì´í„° ì—†ìŒ")
                    
        except Exception as e:
            logger.error(f"ì „ì²´ ìˆœìœ„ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    def analyze_journalist_trends(self, days: int = 7) -> Dict[str, Any]:
        """ê¸°ì í™œë™ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            logger.info(f"ìµœê·¼ {days}ì¼ê°„ ê¸°ì í™œë™ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
            
            # ìµœê·¼ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ
            recent_articles = db_manager.get_articles(
                limit=1000,
                source='ì‹œì‚¬ì˜¤ëŠ˜'
            )
            
            # ë‚ ì§œë³„ ê¸°ì í™œë™ ë¶„ì„
            journalist_activity = {}
            cutoff_date = datetime.now() - timedelta(days=days)
            
            for article in recent_articles:
                if not article.get('author') or not article.get('published_date'):
                    continue
                
                published_date = article['published_date']
                if isinstance(published_date, str):
                    try:
                        published_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                    except:
                        continue
                
                if published_date < cutoff_date:
                    continue
                
                author = article['author']
                date_key = published_date.strftime('%Y-%m-%d')
                
                if author not in journalist_activity:
                    journalist_activity[author] = {}
                
                if date_key not in journalist_activity[author]:
                    journalist_activity[author][date_key] = 0
                
                journalist_activity[author][date_key] += 1
            
            # ê°€ì¥ í™œë°œí•œ ê¸°ìë“¤ ì°¾ê¸°
            total_activity = {author: sum(days.values()) for author, days in journalist_activity.items()}
            top_active = sorted(total_activity.items(), key=lambda x: x[1], reverse=True)[:10]
            
            return {
                'journalist_activity': journalist_activity,
                'top_active_journalists': top_active,
                'analysis_period': f"{days}ì¼",
                'total_journalists_analyzed': len(journalist_activity)
            }
            
        except Exception as e:
            logger.error(f"ê¸°ì íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def print_trend_analysis(self, days: int = 7):
        """íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        trends = self.analyze_journalist_trends(days)
        
        if not trends:
            print(f"\nâŒ ìµœê·¼ {days}ì¼ê°„ íŠ¸ë Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“ˆ ìµœê·¼ {days}ì¼ê°„ ê¸°ì í™œë™ íŠ¸ë Œë“œ")
        print("=" * 60)
        
        top_active = trends.get('top_active_journalists', [])
        if top_active:
            print(f"\nğŸ”¥ ê°€ì¥ í™œë°œí•œ ê¸°ì TOP 10:")
            print("-" * 40)
            print(f"{'ìˆœìœ„':<4} {'ê¸°ìëª…':<15} {'ê¸°ì‚¬ìˆ˜':<8}")
            print("-" * 40)
            
            for i, (name, count) in enumerate(top_active, 1):
                if i == 1:
                    print(f"ğŸ¥‡ {i:<2} {name:<15} {count:<8}")
                elif i == 2:
                    print(f"ğŸ¥ˆ {i:<2} {name:<15} {count:<8}")
                elif i == 3:
                    print(f"ğŸ¥‰ {i:<2} {name:<15} {count:<8}")
                else:
                    print(f"   {i:<2} {name:<15} {count:<8}")
        
        print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"  - ë¶„ì„ ê¸°ê°„: {trends.get('analysis_period', 'N/A')}")
        print(f"  - ë¶„ì„ëœ ê¸°ì ìˆ˜: {trends.get('total_journalists_analyzed', 0)}ëª…")
    
    def get_journalist_insights(self, journalist_name: str) -> Dict[str, Any]:
        """íŠ¹ì • ê¸°ìì— ëŒ€í•œ ìƒì„¸ ì¸ì‚¬ì´íŠ¸"""
        try:
            # ê¸°ìë³„ í†µê³„ ì¡°íšŒ
            journalist_stats = db_manager.get_journalist_stats_by_journalist(journalist_name)
            
            if not journalist_stats:
                return {}
            
            # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
            category_distribution = {}
            total_articles = 0
            
            for stat in journalist_stats:
                category = stat['category']
                count = stat['article_count']
                category_distribution[category] = count
                total_articles += count
            
            # ì£¼ìš” ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
            if category_distribution:
                main_category = max(category_distribution.items(), key=lambda x: x[1])
                main_category_name = main_category[0]
                main_category_count = main_category[1]
                main_category_ratio = (main_category_count / total_articles) * 100
            else:
                main_category_name = "N/A"
                main_category_count = 0
                main_category_ratio = 0
            
            # ê¸°ì ì •ë³´ ì¡°íšŒ
            journalist_info = db_manager.get_journalist_info(journalist_name, source='ì‹œì‚¬ì˜¤ëŠ˜')
            
            return {
                'journalist_name': journalist_name,
                'total_articles': total_articles,
                'category_distribution': category_distribution,
                'main_category': {
                    'name': main_category_name,
                    'count': main_category_count,
                    'ratio': main_category_ratio
                },
                'category_count': len(category_distribution),
                'journalist_info': journalist_info,
                'stats': journalist_stats
            }
            
        except Exception as e:
            logger.error(f"ê¸°ì ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def print_journalist_insights(self, journalist_name: str):
        """ê¸°ì ì¸ì‚¬ì´íŠ¸ ì¶œë ¥"""
        insights = self.get_journalist_insights(journalist_name)
        
        if not insights:
            print(f"\nâŒ '{journalist_name}' ê¸°ìì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ‘¤ {journalist_name} ê¸°ì ìƒì„¸ ë¶„ì„")
        print("=" * 60)
        
        print(f"ğŸ“Š ê¸°ë³¸ ì •ë³´:")
        print(f"  - ì´ ê¸°ì‚¬ ìˆ˜: {insights['total_articles']}ê°œ")
        print(f"  - í™œë™ ì¹´í…Œê³ ë¦¬: {insights['category_count']}ê°œ")
        print(f"  - ì£¼ìš” ì¹´í…Œê³ ë¦¬: {insights['main_category']['name']} ({insights['main_category']['count']}ê°œ, {insights['main_category']['ratio']:.1f}%)")
        
        print(f"\nğŸ“° ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ë¶„í¬:")
        category_dist = insights['category_distribution']
        if category_dist:
            sorted_categories = sorted(category_dist.items(), key=lambda x: x[1], reverse=True)
            for category, count in sorted_categories:
                ratio = (count / insights['total_articles']) * 100
                print(f"  - {category}: {count}ê°œ ({ratio:.1f}%)")
        
        # ìµœê·¼ í™œë™ ì •ë³´
        if insights.get('journalist_info'):
            info = insights['journalist_info']
            if info.get('last_article_date'):
                print(f"\nâ° ìµœê·¼ í™œë™:")
                print(f"  - ë§ˆì§€ë§‰ ê¸°ì‚¬: {info['last_article_date'].strftime('%Y-%m-%d %H:%M')}")
            if info.get('first_article_date'):
                print(f"  - ì²« ê¸°ì‚¬: {info['first_article_date'].strftime('%Y-%m-%d %H:%M')}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì‹œì‚¬ì˜¤ëŠ˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ê¸°ì ìˆœìœ„ ì‹œìŠ¤í…œ')
    parser.add_argument('--mode', choices=['crawl', 'rank', 'trend', 'insight', 'all'], 
                       default='all', help='ì‹¤í–‰ ëª¨ë“œ ì„ íƒ')
    parser.add_argument('--pages', type=int, default=1, 
                       help='ì¹´í…Œê³ ë¦¬ë‹¹ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)')
    parser.add_argument('--workers', type=int, default=3, 
                       help='ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 3)')
    parser.add_argument('--category', choices=['ì •ì¹˜', 'ê²½ì œ', 'ì‚°ì—…', 'ê±´ì„¤Â·ë¶€ë™ì‚°', 'IT', 'ìœ í†µÂ·ë°”ì´ì˜¤', 'ì‚¬íšŒ', 'ìë™ì°¨'], 
                       help='íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§')
    parser.add_argument('--journalist', type=str, 
                       help='íŠ¹ì • ê¸°ì ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ')
    parser.add_argument('--trend-days', type=int, default=7, 
                       help='íŠ¸ë Œë“œ ë¶„ì„ ê¸°ê°„ (ì¼, ê¸°ë³¸ê°’: 7)')
    parser.add_argument('--force-refresh', action='store_true', 
                       help='ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ìƒˆë¡œê³ ì¹¨')
    
    args = parser.parse_args()
    
    logger.info("ì‹œì‚¬ì˜¤ëŠ˜ í¬ë¡¤ë§ ë° ìˆœìœ„ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    db_connected = db_manager.initialize_pool()
    if not db_connected:
        logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - í¬ë¡¤ë§ë§Œ ì§„í–‰í•©ë‹ˆë‹¤")
    
    # í…Œì´ë¸” ìƒì„± (í•„ìš”í•œ ê²½ìš°)
    if db_connected:
        db_manager.create_tables()
    
    ranking_system = JournalistRankingSystem()
    
    if args.mode in ['crawl', 'all']:
        print("ğŸš€ 1ë‹¨ê³„: ì‹œì‚¬ì˜¤ëŠ˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
        crawler = SisaonCrawler(max_workers=args.workers)
        
        if args.category:
            # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§
            if args.category in crawler.categories:
                category_code = crawler.categories[args.category]
                crawler.crawl_category(args.category, category_code, args.pages)
            else:
                print(f"âŒ ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬: {args.category}")
        else:
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
            crawler.crawl_all_categories(max_pages_per_category=args.pages)
    
    if args.mode in ['rank', 'all'] and db_connected:
        print("\nğŸ“Š 2ë‹¨ê³„: ê¸°ì í†µê³„ ìƒì„±")
        if ranking_system.generate_journalist_stats(force_refresh=args.force_refresh):
            print("âœ… ê¸°ì í†µê³„ ìƒì„± ì™„ë£Œ")
            
            # 3ë‹¨ê³„: ìˆœìœ„ ì¶œë ¥
            print("\nğŸ† 3ë‹¨ê³„: ê¸°ì ìˆœìœ„ ì¶œë ¥")
            ranking_system.print_all_rankings(limit=10)
            
            # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ì¶œë ¥
            if args.category:
                ranking_system.print_category_rankings(args.category, limit=10)
            else:
                ranking_system.print_category_rankings('ì •ì¹˜', limit=10)
                ranking_system.print_category_rankings('ê²½ì œ', limit=10)
        else:
            print("âŒ ê¸°ì í†µê³„ ìƒì„± ì‹¤íŒ¨")
    
    if args.mode in ['trend', 'all'] and db_connected:
        print(f"\nğŸ“ˆ 4ë‹¨ê³„: ê¸°ì í™œë™ íŠ¸ë Œë“œ ë¶„ì„ (ìµœê·¼ {args.trend_days}ì¼)")
        ranking_system.print_trend_analysis(days=args.trend_days)
    
    if args.mode == 'insight' and args.journalist and db_connected:
        print(f"\nğŸ‘¤ 5ë‹¨ê³„: {args.journalist} ê¸°ì ìƒì„¸ ë¶„ì„")
        ranking_system.print_journalist_insights(args.journalist)
    
    # ì¶”ê°€ í†µê³„ ì •ë³´ ì¶œë ¥
    if db_connected and args.mode in ['rank', 'all']:
        print("\nğŸ“‹ ì¶”ê°€ í†µê³„ ì •ë³´:")
        print("-" * 40)
        
        # ì „ì²´ í†µê³„
        stats = db_manager.get_crawling_statistics()
        if stats:
            print(f"ğŸ“° ì „ì²´ ê¸°ì‚¬ ìˆ˜: {stats.get('total_articles', 0)}ê°œ")
            print(f"ğŸ“… ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {stats.get('today_articles', 0)}ê°œ")
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            articles_by_source = stats.get('articles_by_source', {})
            if articles_by_source:
                print(f"ğŸ“Š ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ë¶„í¬:")
                for source, count in articles_by_source.items():
                    print(f"  - {source}: {count}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬ë„
        category_dist = db_manager.get_category_distribution()
        if category_dist and category_dist.get('category_stats'):
            print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            for cat_stat in category_dist['category_stats'][:5]:  # ìƒìœ„ 5ê°œë§Œ
                print(f"  - {cat_stat['category']}: {cat_stat['total_articles']}ê°œ ({cat_stat['journalist_count']}ëª… ê¸°ì)")
    
    logger.info("ì‹œì‚¬ì˜¤ëŠ˜ í¬ë¡¤ë§ ë° ìˆœìœ„ ì‹œìŠ¤í…œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main() 